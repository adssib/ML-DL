{"cells":[{"cell_type":"markdown","metadata":{"id":"S02d9PimAZKJ"},"source":["In this lab we will go over over basic stochastic optimization and uses in pytorch\n","\n"]},{"cell_type":"markdown","metadata":{"id":"CIDPxv5l2qpI"},"source":["(1) Setup the MNIST dataloaders for both the training (and  now as well test) set as in Lab 3. You do not need to iterate through the dataloaders, these will be used in the rest of the lab.\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"HIv6h0xwUWyx"},"outputs":[],"source":["#This cell does not require outputing anything but is setup for subsequent cells\n"]},{"cell_type":"markdown","metadata":{"id":"14MBuiusdZmA"},"source":["(2) Consider your work from [Lab 3 Excercise 2]. Modify the neural network as follows. The hidden outputs should be size 100 in both hidden layers and the activations should be relu. Modify the final layer to output 10 values instead of 1.\n","\n","Your code should implement $f(x) = W_2\\rho(W_1\\rho(W_0x+b_0)+b_1)+b_2$ with $f: R^{784}-> R^{10}$ and $\\rho = relu$\n","\n","Initialize the weights  using a variant of xavier intialization $w_{ij} \\sim N(0,\\frac{1}{\\sqrt{n_i}})$ where $n_i$ is the size of the layer input. Initialize the biases as 0. Write a helper function to perform this initialization for subsequent parts of this lab"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"bhZ0I_q7dnb9"},"outputs":[],"source":["import torch\n","\n","## Initialize and track the parameters using a list or dictionary\n","param_dict = {\n","    \"W0\": None,\n","    \"b0\": None,\n","    \"W1\": None,\n","    \"b1\": None,\n","    \"W2\": None,\n","    \"b2\": None,\n","    }\n","\n","## Define the network\n","def my_nn(input, param_dict):\n","    r\"\"\"Performs a single forward pass of a Neural Network with the given\n","    parameters in param_dict.\n","\n","    Args:\n","        input (torch.tensor): Batch of images of shape (B, H, W), where B is\n","            the number of input samples,and H and W are the image height and\n","            width respectively.\n","        param_dict (dict of torch.tensor): Dictionary containing the parameters\n","            of the neural network. Expects dictionary keys to be of format\n","            \"W#\" and \"b#\" where # is the layer number.\n","\n","    Returns:\n","        torch.tensor: Neural network output of shape (B, )\n","    \"\"\"\n","    pass\n","\n","\n","def initialize_nn(param_dict):\n","  \"\"\"Takes a dictionary with existing torch tensors\n","      and re-initializes them using xavier initialization\n","  \"\"\"\n","  pass"]},{"cell_type":"markdown","metadata":{"id":"VTDfIZSOk1mF"},"source":["We will evaluate the cross entropy loss and average accuracy on this randomly initialized dataset. Use the torch.nn.functional.cross_entropy() function to compute the loss  $\\frac{1}{N}\\sum_i^N CrossEntropy(f(x_i),y_i)$ and accuracy. Your accuracy should be close to $10\\%$ as the network has random weights. Note the pytorch cross_entropy function already applies the softmax operator so you do not need to apply this just feed the model output directly  \n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"vq3Zh5A_lBOV"},"outputs":[],"source":["cuda = torch.cuda.is_available()\n","\n","for data, target in test_dataloader:\n","  #move to GPU if available\n","  if cuda:\n","    data, target = data.to('cuda'), target.to('cuda')\n","\n","  #compute model output\n","\n","  #comptue accuracy for minibatch\n","\n","  #compute loss for minibatch\n","\n","#aggregate loss and accuracy for all test data\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"jqKxv88wTYFo"},"source":["(3) (a) Without any use of the torch.optim package implement from scratch mini-batch Stochastic Gradient Descent training to minimize the loss  $\\frac{1}{N}\\sum_i^N CrossEntropy(f(x_i),y_i)$ over the MNIST dataset. Use a minibatch size of 128 and a learning rate of 0.01 and run training for 20 epochs.  \n","\n","You will use torch autograd features (e.g., .backward()) to obtain the gradients given each mini-batch at each parameter and then modify the existing parameters based on this gradient.\n","\n","Store the training loss and accuracy of each minibatch and plot each of these (with iterations (not epochs) as the x-axis). You can optionally smooth out these plots over 20-100 iteration window of your choosing to make them cleaner to read. Compute and store the test accuracy at the end of each epoch and plot the per-epoch train and test accuracies together.\n","\n","You should end up with 3 plots.\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"S4Lw5vyAegCC"},"outputs":[],"source":["import torch\n","\n","\n","train_losses = [] # use to append the avg loss for each minibatch\n","train_accs = [] # use to append the avg acc of minibatch\n","\n","alpha=0.01\n","\n","\n","for epoch in range(20):\n","  for  ... #Iterate over dataset\n","    #Pass the data through the model and get the output predictions\n","\n","    #Compute the gradient for the minibatch\n","\n","    #Update the model parameters by w_t-alpha*\\grad_{w_t}(f(w_t))\n","\n","    #The following code will clear the gradient buffers for the next iteration\n","    for (_,param) in param_dict.items():\n","      if param.grad is not None:\n","        param.grad.detach_()\n","        param.grad.zero_()\n","\n","    #Update loss and acc tracking\n","\n","  #Evaluate on the test set\n","\n","#Plot the train loss and acc\n","\n","#Plot the per_epoch train and test acc"]},{"cell_type":"markdown","metadata":{"id":"mo_i-btpZw-m"},"source":["(4) Repeat (3) but now using the package torch.optim.SGD to perform SGD.\n","\n","\n","https://pytorch.org/docs/stable/optim.html?highlight=torch%20optim%20sgd#torch.optim.SGD\n","\n","\n","Plot the same training curves and accuracy curves. Check that your learning curves are similar to those in (3). You can for example overlay them or plot them side by side with subplot.\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"CCnQoLCXrpBt"},"outputs":[],"source":["#Answer for SGD\n","import torch\n","#Make sure to reinitialize your network to random before starting training\n","initialize_nn(param_dict)\n","\n","#optim.SGD takes a list of parameters which you can get from your dictionary as follows\n","parameter_list = param_dict.values()\n","\n","optimizer = torch.optim.SGD(parameter_list, lr=alpha)\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"rYqVuiN0dK6z"},"source":["(Optional) Modify the code from (3) and (4)  to perform minibatch SGD with momentum. Use a momentum of $\\mu=0.9$ and learning rate $\\alpha=0.01$. Use the following formulation of momentum:\n","\n","$g = \\nabla_wCE(f(w_t,X),Y)$  gradient estimate with mini-batch\n","\n","$v_{t+1} = \\mu * v_t + g$\n","\n","$w_{t+1} = w - \\alpha*v_{t+1}$\n","\n","Obtain the same plots as before and a final test accuracy"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"eAI69yxnrA-S"},"outputs":[],"source":["#Reinitialize your network to random!\n","initialize_nn(param_dict)\n","\n","losses = [] # use to append the avg loss for each minibatch\n","train_acc = [] # use to append the avg acc of minibatch\n","\n","alpha=0.01\n","mu=0.9\n","\n","for epoch in range(20):\n","  for ... #Iterate over dataset\n","    #Compute the gradient for the minibatch\n","\n","    #Update the model parameters as described above\n","\n","    #The following code will clear the gradient buffers for the next iteration\n","    for (_,param) in param_dict.items():\n","      if param.grad is not None:\n","        param.grad.detach_()\n","        param.grad.zero_()\n","\n","    #Update loss and acc tracking\n","\n","  #Evaluate on the test set\n","\n","#Plot the train loss and acc\n","\n","#Plot the per_epoch train and test acc"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"fy8p8DUIkT8m"},"outputs":[],"source":["#Answer for SGD+momentum"]}],"metadata":{"accelerator":"GPU","colab":{"provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"}},"nbformat":4,"nbformat_minor":0}
